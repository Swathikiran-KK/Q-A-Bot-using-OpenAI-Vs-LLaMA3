# Q&A Bot Benchmark: OpenAI vs LLaMA 3.1 (Groq)

This project provides a simple and clear way to **compare answers from two different AI models**:

- **OpenAI (via OpenRouter)** using `gpt-4o-mini` for both text and image understanding.
- **LLaMA 3.1 (Groq)** using `llama-3.1-8b-instant` for fast, free text generation.

The goal is to help you **see which model performs better** across different tasks such as answering text prompts, understanding images, summarizing documents, and answering questions based on PDF content. The system also includes a voting feature and an analytics dashboard that summarizes performance and gives an **overall winner**.

---
## ğŸ–¥ï¸ App Preview

<p align="center">
  <img src="Output/output1.png" width="700"/>
</p>

## Features

1. **Text Comparison**
   - Enter any prompt and see both models' responses side-by-side.
   - Compare metrics like readability, length, citations, and latency.

2. **Image â†’ Text Comparison**
   - Upload an image.
   - OpenAI sees the image and responds.
   - LLaMA responds only based on the prompt (baseline comparison).

3. **Document â†’ Text Comparison**
   - Upload PDF, DOCX, TXT, or CSV.
   - Both models summarize or extract information.

4. **RAG (Retrieval-Augmented Generation)**
   - Upload a PDF.
   - The system extracts text and retrieves the most relevant chunks.
   - Both models answer based on that extracted context.
   - Measures grounding and citation coverage.

5. **User Preference Voting**
   - After each comparison, you choose which answer you prefer.

6. **Analytics Dashboard**
   - Shows latency charts, readability comparison, grounding coverage, and vote counts.
   - Produces an **overall performance score and declares a winner**.

---

## Folder Structure

```
qa_benchmark/
â”œâ”€â”€ app.py
â”œâ”€â”€ README.md
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 1_Text_Compare.py
â”‚   â”œâ”€â”€ 2_Multimodal_Compare.py
â”‚   â”œâ”€â”€ 3_RAG_Compare.py
â”‚   â”œâ”€â”€ 4_Analytics.py
â”‚   â””â”€â”€ 5_Settings.py
â”œâ”€â”€ components/
â”‚   â””â”€â”€ ui.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ openrouter.py
â”‚   â”œâ”€â”€ groq_llama.py
â”‚   â”œâ”€â”€ embeddings_jina.py
â”‚   â””â”€â”€ vectordb_qdrant.py
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ document_processor.py
â”‚   â””â”€â”€ hybrid_retriever.py
â”œâ”€â”€ evaluators/
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ analytics/
â”‚   â””â”€â”€ tracker.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```



---

Key Links:

| Service | Link |
|--------|------|
| OpenRouter Keys | https://openrouter.ai/keys |
| Groq Keys | https://console.groq.com/keys |

---

## Running the App

streamlit run app.py


Navigate using the sidebar:
- Text Compare
- Images & Documents
- RAG Compare
- Analytics

---

## How the Evaluation Works

The system compares models on multiple dimensions:

| Category | What It Measures |
|--------|------------------|
| Reasoning | How clearly and logically the answer is written |
| Readability | How easy the text is to read |
| Grounding (RAG) | How much of the answer comes from actual document content |
| Latency | How fast the model responds |
| User Preference | Your vote on which answer you like more |

These are combined in the analytics dashboard to produce a **final performance score** and **declare an overall winner**.


---
## Comparison

<p align="center">
  <img src="Output/output9.png" width="700"/>
  <img src="Output/output10.png" width="700"/>
  <img src="Output/output11.png" width="700"/>
  <img src="Output/output12.png" width="700"/>
</p>


## Closing Note

This project is designed to help you **understand model strengths clearly and visually** instead of guessing. It is especially useful for:

- Picking the best model for your use-case
- Demonstrating model behavior to teams/clients
- Performing lightweight model benchmarking without cost
