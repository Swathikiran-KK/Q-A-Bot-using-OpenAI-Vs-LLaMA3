# Q&A Bot Benchmark: OpenAI vs LLaMA 3.1 (Groq)

This project provides a simple and clear way to **compare answers from two different AI models**:

- **OpenAI (via OpenRouter)** using `gpt-4o-mini` for both text and image understanding.
- **LLaMA 3.1 (Groq)** using `llama-3.1-8b-instant` for fast, free text generation.

The goal is to help you **see which model performs better** across different tasks such as answering text prompts, understanding images, summarizing documents, and answering questions based on PDF content. The system also includes a voting feature and an analytics dashboard that summarizes performance and gives an **overall winner**.

---

## Features

1. **Text Comparison**
   - Enter any prompt and see both models' responses side-by-side.
   - Compare metrics like readability, length, citations, and latency.

2. **Image â†’ Text Comparison**
   - Upload an image.
   - OpenAI sees the image and responds.
   - LLaMA responds only based on the prompt (baseline comparison).

3. **Document â†’ Text Comparison**
   - Upload PDF, DOCX, TXT, or CSV.
   - Both models summarize or extract information.

4. **RAG (Retrieval-Augmented Generation)**
   - Upload a PDF.
   - The system extracts text and retrieves the most relevant chunks.
   - Both models answer based on that extracted context.
   - Measures grounding and citation coverage.

5. **User Preference Voting**
   - After each comparison, you choose which answer you prefer.

6. **Analytics Dashboard**
   - Shows latency charts, readability comparison, grounding coverage, and vote counts.
   - Produces an **overall performance score and declares a winner**.

---

## Folder Structure

## ğŸ“ Folder Structure


ğŸ“‚ qa_benchmark
â”£ ğŸ“„ app.py â†’ Main homepage + navigation
â”£ ğŸ“„ README.md
â”ƒ
â”£ ğŸ“‚ pages (Streamlit multipage UI)
â”ƒ â”£ ğŸ“ 1_Text_Compare.py â†’ OpenAI vs LLaMA-3.1 text comparison + voting
â”ƒ â”£ ğŸ–¼ï¸ 2_Multimodal_Compare.py â†’ Imageâ†’Text (OpenAI vision) & Documentâ†’Text + voting
â”ƒ â”£ ğŸ“š 3_RAG_Compare.py â†’ PDF â†’ RAG (Hybrid + Vector Search) + citations + voting
â”ƒ â”£ ğŸ“Š 4_Analytics.py â†’ Metrics dashboard + vote counts + final winner + reset
â”ƒ â”— âš™ï¸ 5_Settings.py â†’ Display model configuration & environment status
â”ƒ
â”£ ğŸ“‚ components
â”ƒ â”— ğŸ¨ ui.py â†’ UI layout helpers (headers, metric cards, answer blocks)
â”ƒ
â”£ ğŸ“‚ services
â”ƒ â”£ ğŸ¤– openrouter.py â†’ GPT-4o-mini via OpenRouter (text + vision)
â”ƒ â”£ âš¡ groq_llama.py â†’ LLaMA-3.1 via Groq (super-fast text)
â”ƒ â”£ ğŸ”¤ embeddings_jina.py â†’ (Optional) Jina embeddings for vector DB
â”ƒ â”— ğŸ—„ï¸ vectordb_qdrant.py â†’ (Optional) Qdrant vector DB (auto-fallback to TF-IDF)
â”ƒ
â”£ ğŸ“‚ retrieval
â”ƒ â”£ ğŸ“„ document_processor.py â†’ PDF / DOCX / TXT / CSV text extraction & chunking
â”ƒ â”— ğŸ§­ hybrid_retriever.py â†’ BM25 + TF-IDF hybrid retriever (torch-free)
â”ƒ
â”£ ğŸ“‚ evaluators
â”ƒ â”— ğŸ“ metrics.py â†’ Readability, grounding coverage, citations, token estimates
â”ƒ
â”£ ğŸ“‚ analytics
â”ƒ â”— ğŸ§¾ tracker.py â†’ run_id-based logging + CSV persistence + vote storage
â”ƒ
â”£ ğŸ“‚ utils
â”ƒ â”— ğŸ”§ config.py â†’ Model names, API keys, cost map
â”ƒ
â”£ ğŸ“¦ requirements.txt â†’ Dependencies
â”— ğŸ”’ .env â†’ Your keys

Key Links:

| Service | Link |
|--------|------|
| OpenRouter Keys | https://openrouter.ai/keys |
| Groq Keys | https://console.groq.com/keys |

---

## Running the App

streamlit run app.py


Navigate using the sidebar:
- Text Compare
- Images & Documents
- RAG Compare
- Analytics

---

## How the Evaluation Works

The system compares models on multiple dimensions:

| Category | What It Measures |
|--------|------------------|
| Reasoning | How clearly and logically the answer is written |
| Readability | How easy the text is to read |
| Grounding (RAG) | How much of the answer comes from actual document content |
| Latency | How fast the model responds |
| User Preference | Your vote on which answer you like more |

These are combined in the analytics dashboard to produce a **final performance score** and **declare an overall winner**.


---

## Closing Note

This project is designed to help you **understand model strengths clearly and visually** instead of guessing. It is especially useful for:

- Picking the best model for your use-case
- Demonstrating model behavior to teams/clients
- Performing lightweight model benchmarking without cost
